{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce13a6b7-9c84-4061-ab14-b066b01f563e",
   "metadata": {},
   "source": [
    "# Reading the Reivews and the Product Medatdata Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "714519a2-979f-4390-bb06-cea2dbbde64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7867e701-751d-4e61-87f0-1cbad6331a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3391\n",
      "71497\n"
     ]
    }
   ],
   "source": [
    "# Function to load a .jsonl file\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load the datasets\n",
    "reviews_df = load_jsonl('Magazine_Subscriptions.jsonl')\n",
    "metadata_df = load_jsonl('meta_Magazine_Subscriptions.jsonl')\n",
    "print(len(metadata_df))\n",
    "print(len(reviews_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16892f4c-6120-4567-873a-b2410bef799a",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "488aca12-1e73-483d-b62a-693e20a87911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d67d9cf-4e34-48af-ad03-a82f1fe702ad",
   "metadata": {},
   "source": [
    "## Filter duplicates and not verified review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4b9c3f3-fc8f-46c2-b66a-099fb3315ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "medadata_df = metadata_df.drop_duplicates(subset=['parent_asin']) # drop duplication row\n",
    "reviews_df = reviews_df[reviews_df['verified_purchase'] == True] # only keep the review from verified purchase\n",
    "reviews_df['review_date'] = pd.to_datetime(reviews_df['timestamp'], unit='ms')\n",
    "medadata_df.rename(columns={'title': 'product_title'}, inplace=True)\n",
    "reviews_df.rename(columns={'title': 'review_title'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "119f2be4-88a6-461c-9d22-08571ae86c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['main_category', 'product_title', 'average_rating', 'rating_number',\n",
      "       'features', 'description', 'price', 'images', 'videos', 'store',\n",
      "       'categories', 'details', 'parent_asin', 'bought_together', 'subtitle',\n",
      "       'author'],\n",
      "      dtype='object')\n",
      "Index(['rating', 'review_title', 'text', 'images', 'asin', 'parent_asin',\n",
      "       'user_id', 'timestamp', 'helpful_vote', 'verified_purchase',\n",
      "       'review_date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(medadata_df.columns)\n",
    "print(reviews_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3cfc6b-2a1d-4720-9eb8-e9a4da4e7a33",
   "metadata": {},
   "source": [
    "## Join the two datasets togather by parent_asin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b79d72d3-4b79-42c6-b49b-664c0290f190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                      Cooking With Paula Deen\n",
      "1    Sports Illustrated KIDS    Print Magazine\n",
      "2                    Outside    Print Magazine\n",
      "3                  Us Weekly    Print Magazine\n",
      "4      O, The Oprah Magazine    Print Magazine\n",
      "Name: title, dtype: object\n",
      "Index(['rating', 'review_title', 'text', 'images_x', 'asin', 'parent_asin',\n",
      "       'user_id', 'timestamp', 'helpful_vote', 'verified_purchase',\n",
      "       'review_date', 'main_category', 'title', 'average_rating',\n",
      "       'rating_number', 'features', 'description', 'price', 'images_y',\n",
      "       'videos', 'store', 'categories', 'details', 'bought_together',\n",
      "       'subtitle', 'author'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "combined_df = pd.merge(reviews_df, metadata_df, on='parent_asin', how='left') # join the review and the product metadata\n",
    "print(combined_df['title'].head())\n",
    "print(combined_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53316852-3488-4a13-99d8-7f03be787f82",
   "metadata": {},
   "source": [
    "## Join the two datasets process the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18cfa1e9-8899-432d-8c82-e03faf2034ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['review_content'] = combined_df.apply(lambda x: f\"Product Title: {x['title']}, Rating: {x['rating']}, Review Title: {x['review_title']}, Review Text: {x['text']}, Reiview Data: {x['review_date']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9428414d-60af-4391-ac0f-f4d0367f6f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Product Title: Cooking With Paula Deen, Rating...\n",
      "1    Product Title: Sports Illustrated KIDS    Prin...\n",
      "2    Product Title: Outside    Print Magazine, Rati...\n",
      "3    Product Title: Us Weekly    Print Magazine, Ra...\n",
      "4    Product Title: O, The Oprah Magazine    Print ...\n",
      "Name: review_content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(combined_df['review_content'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8beb4ed3-a6ec-49cc-b7db-2cebb3345bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将所有评论聚合为一个列表，按照每个产品（parent_asin）\n",
    "reviews_grouped = combined_df.groupby('parent_asin')['review_content'].apply(list).reset_index()\n",
    "\n",
    "# 将列表转换为字符串，每条评论之间用新行分隔\n",
    "reviews_grouped['reviews'] = reviews_grouped['review_content'].apply(lambda x: '\\n'.join(x))\n",
    "\n",
    "# 将这个聚合后的评论数据合并回产品元数据中\n",
    "final_metadata_df = pd.merge(metadata_df, reviews_grouped[['parent_asin', 'reviews']], on='parent_asin', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "785711bf-e321-49fe-abb6-6d45f5f93378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['main_category', 'title', 'average_rating', 'rating_number', 'features',\n",
      "       'description', 'price', 'images', 'videos', 'store', 'categories',\n",
      "       'details', 'parent_asin', 'bought_together', 'subtitle', 'author',\n",
      "       'reviews'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(final_metadata_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ea0c12a-897d-4317-8530-eaa96fe93dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_features(features):\n",
    "    if isinstance(features, list):\n",
    "        return ', '.join(features)\n",
    "    return ''\n",
    "\n",
    "def format_description(description):\n",
    "    if isinstance(description, list):\n",
    "        return ' '.join(description)\n",
    "    return ''\n",
    "\n",
    "def format_details(details):\n",
    "    if isinstance(details, dict):\n",
    "        return ', '.join([f\"{k}: {v}\" for k, v in details.items()])\n",
    "    return ''\n",
    "\n",
    "# 格式化文本\n",
    "def format_row(row):\n",
    "    return (\n",
    "        f\"Title: {row['title']}, \"\n",
    "        f\"Subtitle: {row.get('subtitle', '')}, \"\n",
    "        f\"Features: {format_features(row['features'])}, \"\n",
    "        f\"Description: {format_description(row['description'])}, \"\n",
    "        f\"Average Rating: {row['average_rating']}, \"\n",
    "        f\"Price: ${row['price']}, \"\n",
    "        f\"Store: {row['store']}, \"\n",
    "        f\"Categories: {', '.join(row.get('categories', []))}, \"\n",
    "        f\"Details: {format_details(row['details'])}, \"\n",
    "        f\"Author: {row.get('author', '')}, \"\n",
    "        f\"Reviews: {row['reviews']}\"\n",
    "    )\n",
    "\n",
    "# 应用格式化函数到每一行\n",
    "final_metadata_df['formatted_text'] = final_metadata_df.apply(format_row, axis=1)\n",
    "\n",
    "# 创建新的DataFrame仅包含formatted_text\n",
    "result_df = final_metadata_df[['formatted_text']]\n",
    "\n",
    "# 保存到本地文件\n",
    "result_df.to_csv('formatted_metadata.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af1ea280-00bf-4fdd-a0f0-8cd6017fd8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# 将DataFrame变量设为None来释放引用\n",
    "reviews_df = None\n",
    "metadata_df = None\n",
    "combined_df = None\n",
    "final_metadata_df = None\n",
    "result_df = None\n",
    "\n",
    "# 强制进行垃圾回收\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b425f-bc23-43ad-af1f-429223e373a3",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "373b2ffe-be4b-4220-b804-932715269ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d173f79a-761b-4964-8479-ccef9f53132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_metadata = 'formatted_metadata.csv'\n",
    "\n",
    "# 读取文件\n",
    "# 因为文件没有列名，我们可以使用header=None让pandas不把第一行作为列名\n",
    "# 并且可以通过names参数指定列名，这里我们将其命名为'formatted_text'\n",
    "final_df = pd.read_csv(formatted_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8253c022-7340-43a4-8a0b-a658c670bd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2', use_fast=True)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdb51807-fbe8-4a4f-a17c-e28ae31d3605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, max_length=1024):\n",
    "    # 使用tokenizer的encode方法获取token ids，这里不直接生成tensor\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    # 按照max_length拆分token ids\n",
    "    chunks = [token_ids[i:i + max_length] for i in range(0, len(token_ids), max_length)]\n",
    "    \n",
    "    # 将token ids列表转换回文本\n",
    "    split_texts = [tokenizer.decode(chunk, clean_up_tokenization_spaces=True) for chunk in chunks]\n",
    "    \n",
    "    return split_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c33a693-9e55-40f1-9496-8c4e05b68d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3412 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# 应用拆分函数\n",
    "split_results = final_df['formatted_text'].apply(split_text)\n",
    "\n",
    "# 展开拆分结果为新的DataFrame行\n",
    "expanded_rows = []\n",
    "for original_index, text_list in enumerate(split_results):\n",
    "    for text in text_list:\n",
    "        expanded_rows.append({'formatted_text': text})\n",
    "\n",
    "expanded_df = pd.DataFrame(expanded_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92e78569-1670-468f-bfe7-a696a2c65b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         formatted_text\n",
      "0     Title: GQ Print Access    Print Magazine, Subt...\n",
      "1     Title: Hi-Fi +    Print Magazine, Subtitle: na...\n",
      "2     Title: Paper Crafts, Subtitle: nan, Features:,...\n",
      "3     Title: Horse Illustrated, Subtitle: nan, Featu...\n",
      "4      with lots of useful information, Review Text:...\n",
      "...                                                 ...\n",
      "7109  Title: Karavan Istorij, Subtitle: nan, Feature...\n",
      "7110  Title: V Magazine - Ny    Print Magazine, Subt...\n",
      "7111  Title: Victorian Review    Print Magazine, Sub...\n",
      "7112  Title: Visto    Print Magazine, Subtitle: nan,...\n",
      "7113  Title: Modern Pioneer    Print Magazine, Subti...\n",
      "\n",
      "[7114 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(expanded_df)\n",
    "dataset = Dataset.from_pandas(expanded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "756d4799-1f0a-408c-8edc-5091ca5a348f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829779d230c64ae6ab94efc4379ffb27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7114 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    # 对文本进行Tokenize处理\n",
    "    tokenized_inputs = tokenizer(examples['formatted_text'], truncation=True, padding='max_length', max_length=512, return_tensors=\"pt\")\n",
    "    # GPT-2的标准训练过程中，labels字段通常与input_ids相同\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].clone()\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "batch_size = 3\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dc264c93-dc10-43a0-ad55-c16b5bff69e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\cse561\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "from transformers import get_scheduler\n",
    "\n",
    "def create_optimizer_and_scheduler(model):\n",
    "    # 定义优化器\n",
    "    optimizer = SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "    # 定义学习率调度器，这里以线性调度器为例\n",
    "    num_training_steps = 500  # 假设的训练步数，你需要根据实际情况设定\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=500,  # 预热步数\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    return optimizer, lr_scheduler\n",
    "\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=4,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    eval_steps=500,\n",
    "    save_total_limit=2,\n",
    "    auto_find_batch_size = True,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "# 使用Trainer进行训练\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,  # 注意这里需要根据实际情况调整\n",
    "    optimizers = create_optimizer_and_scheduler(model)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9ad17541-5566-4d18-88db-365f338c1915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2372' max='2372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2372/2372 28:45, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.652100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.660100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.652300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2372, training_loss=1.6549579691122998, metrics={'train_runtime': 1726.1801, 'train_samples_per_second': 16.485, 'train_steps_per_second': 1.374, 'total_flos': 7435326062592000.0, 'train_loss': 1.6549579691122998, 'epoch': 4.0})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "04b93b5c-4ba4-4d37-aa74-ea81b450b6e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 保存微调后的模型\n",
    "model.save_pretrained(\"./gpt2_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8a972fde-d305-4a88-bf5a-6e025c00404b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommende me a high-rating magazine.\n",
      "\n",
      "I'm not sure if I'm going to be able to do that. I'm not sure if I'm going to be able to do that.\n",
      "\n",
      "I'm not sure if I'm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 确保PyTorch能够使用GPU（如果可用）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载预训练的模型和tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)  # 将模型移动到正确的设备上\n",
    "\n",
    "# 输入文本\n",
    "input_text = \"Recommende me a high-rating magazine\"\n",
    "# 使用tokenizer编码输入文本，添加批次维度，并将其移动到模型所在的设备\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "# 生成文本\n",
    "# 注意：你可以根据需要调整generate方法的参数来控制生成的文本\n",
    "generated_ids = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# 将生成的token ids解码为文本，并移除特殊的token\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 将生成的文本打印到控制台\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "51f2ef67-ceb0-4311-9f5c-6a4da8a27bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9e976e0c-93b2-46d2-bcb0-8601efea0908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommende me a high-rating magazine.\n",
      "\n",
      "I'm not sure if I'm going to be able to do that. I'm not sure if I'm going to be able to do that.\n",
      "\n",
      "I'm not sure if I'm\n"
     ]
    }
   ],
   "source": [
    "# 确保PyTorch能够使用GPU（如果可用）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载预训练的模型和tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)  # 将模型移动到正确的设备上\n",
    "\n",
    "# 输入文本\n",
    "input_text = \"Recommende me a high-rating magazine\"\n",
    "# 使用tokenizer编码输入文本，添加批次维度，并将其移动到模型所在的设备\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "# 生成文本\n",
    "# 注意：你可以根据需要调整generate方法的参数来控制生成的文本\n",
    "generated_ids = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# 将生成的token ids解码为文本，并移除特殊的token\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 将生成的文本打印到控制台\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4ddf3a-ef15-4d4e-8e0c-d6c79a58c7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "cse561"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
